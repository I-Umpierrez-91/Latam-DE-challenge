{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DE Challenge - Ismael Umpierrez"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocesamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para ejecutar este archivo puede ser necesario instalar las siguientes dependencias:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install gdown pandas google.cloud.storage google-cloud-bigquery memory_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gdown\n",
    "import pandas as pd\n",
    "import zipfile\n",
    "from q1_time import q1_time\n",
    "from q1_memory import q1_memory\n",
    "from q1_bigquery import q1_time as q1_bigquery\n",
    "from q2_time import q2_time\n",
    "from q2_memory import q2_memory\n",
    "\n",
    "from pandas.io import gbq\n",
    "from google.cloud import storage, bigquery"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtengo el archivo desde google drive y lo descomprimo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"farmers-protest-tweets-2021-2-4.json\"\n",
    "url = f'https://drive.google.com/uc?id=1ig2ngoXFTxP5Pa8muXo02mDTFexZzsis'\n",
    "extracted_dir = 'working/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descargo de GDrive\n",
    "gdown.download(url, 'working/tweets.zip', quiet=True)\n",
    "\n",
    "# Extraigo el contenido del archivo\n",
    "with zipfile.ZipFile('working/tweets.zip', 'r') as zip_ref:\n",
    "    zip_ref.extractall(extracted_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Subo los datos del archivo a Google Cloud Storage para poder trabajar con √©l desde bigquery."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables para conexi√≥n a Google Cloud Storage\n",
    "project_id = \"dechallenge\"\n",
    "bucket_name = \"dechallenge-tweets\"\n",
    "keyfile_path = \"..\\creds\\dechallenge-51f78ddf0bb6.json\"  # JSON key file\n",
    "\n",
    "storage_client = storage.Client.from_service_account_json(keyfile_path, project=project_id)\n",
    "bucket = storage_client.bucket(bucket_name)\n",
    "\n",
    "# Creo un blob con el nombre del archivo\n",
    "destination_blob_name = file_path\n",
    "blob = bucket.blob(destination_blob_name)\n",
    "\n",
    "# Subo el archivo a Google Cloud Storage\n",
    "blob.upload_from_filename(extracted_dir+file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Subo el archivo que est√° en un blob de GCS a una tabla de bigquery."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El dataset dechallenge.tweets ya existe.\n",
      "Loaded 117405 rows.\n"
     ]
    }
   ],
   "source": [
    "# Crea un cliente de BigQuery\n",
    "client = bigquery.Client.from_service_account_json(keyfile_path, project=project_id)\n",
    "\n",
    "# Especifica el ID del proyecto y el ID del nuevo dataset\n",
    "dataset_id = \"tweets\"\n",
    "\n",
    "# Crea el dataset\n",
    "dataset = bigquery.Dataset(f\"{project_id}.{dataset_id}\")\n",
    "dataset_ref = client.dataset(dataset_id, project=project_id)\n",
    "\n",
    "# Verifica si el dataset ya existe para no volver a crearlo.\n",
    "if not client.get_dataset(dataset_ref, retry=bigquery.DEFAULT_RETRY):\n",
    "    # Si el dataset no existe, crea el dataset\n",
    "    dataset = bigquery.Dataset(dataset_ref)\n",
    "    client.create_dataset(dataset)\n",
    "    print(f\"Dataset {project_id}.{dataset_id} creado con √©xito.\")\n",
    "else:\n",
    "    print(f\"El dataset {project_id}.{dataset_id} ya existe.\")\n",
    "\n",
    "job_config = bigquery.LoadJobConfig(\n",
    "    autodetect=True,\n",
    "    write_disposition=\"WRITE_TRUNCATE\",\n",
    "    source_format=bigquery.SourceFormat.NEWLINE_DELIMITED_JSON,\n",
    "    max_bad_records=10,\n",
    ")\n",
    "uri = \"gs://dechallenge-tweets/farmers-protest-tweets-2021-2-4.json\"\n",
    "\n",
    "load_job = client.load_table_from_uri(\n",
    "    uri,\n",
    "    \"dechallenge.tweets.farmers_protest_tweets\",\n",
    "    location=\"US\",\n",
    "    job_config=job_config,\n",
    ")  # Make an API request.\n",
    "\n",
    "load_job.result()  # Waits for the job to complete.\n",
    "\n",
    "destination_table = client.get_table(\"dechallenge.tweets.farmers_protest_tweets\")\n",
    "print(\"Loaded {} rows.\".format(destination_table.num_rows))\n",
    "\n",
    "# Ejecuta la consulta de BigQuery"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pregunta 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primero quiero ver cuanto demora una soluci√≥n realizada con BigQuery, haciendo uso de las capacidades que nos brinda el procesamiento distribuido en la nube."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(datetime.date(2021, 2, 12), 'RanbirS00614606'), (datetime.date(2021, 2, 13), 'MaanDee08215437'), (datetime.date(2021, 2, 17), 'RaaJVinderkaur'), (datetime.date(2021, 2, 16), 'jot__b'), (datetime.date(2021, 2, 14), 'rebelpacifist'), (datetime.date(2021, 2, 18), 'neetuanjle_nitu'), (datetime.date(2021, 2, 15), 'jot__b'), (datetime.date(2021, 2, 20), 'MangalJ23056160'), (datetime.date(2021, 2, 23), 'Surrypuria'), (datetime.date(2021, 2, 19), 'Preetm91')]\n"
     ]
    }
   ],
   "source": [
    "resultado = q1_bigquery(client)\n",
    "# Imprime la lista de tuplas\n",
    "print(resultado)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como vemos el resultado es muy r√°pido, pero tenemos que tener en cuenta el overhead de subir los archivos al cloud storage y a una tabla de bigquery."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De todas maneras, para el problema 1 quer√≠a utilizar pandas de manera de tener un benchmark contra lo que comparar. Adem√°s quiero tambi√©n ver cual es el uso de memoria de esta soluci√≥n de \"fuerza bruta\" para tener una idea del impacto de la soluci√≥n que optimiza memoria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filename: c:\\Users\\Caruso\\source\\Latam-DE-challenge\\src\\q1_time.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "     6    142.0 MiB    142.0 MiB           1   @profile\n",
      "     7                                         def q1_time(file_path: str) -> List[Tuple[datetime.date, str]]:\n",
      "     8                                             # Lee el archivo json\n",
      "     9   1609.9 MiB   1467.8 MiB           1       df_tweets_source = pd.read_json(file_path, lines=True)\n",
      "    10                                         \n",
      "    11                                             # Tengo que normalizar el objeto user para poder acceder a sus atributos, en particular me interesa el username.\n",
      "    12                                             # Adem√°s, Transformo la columna 'date' a tipo datetime\n",
      "    13   1611.7 MiB      7.2 MiB           2       df_tweets = df_tweets_source.assign(\n",
      "    14   1599.9 MiB    -10.0 MiB           1               userName=pd.json_normalize(df_tweets_source['user']).username,\n",
      "    15   1604.5 MiB      4.6 MiB           1               date=pd.to_datetime(df_tweets_source['date']).dt.date)\n",
      "    16                                         \n",
      "    17                                             # Agrupo por 'userName' y 'date' y cuento las filas para saber cuantos tweets hizo cada usuario por d√≠a.\n",
      "    18   1599.6 MiB    -12.1 MiB           1       df_tweets = df_tweets.groupby(['userName','date']).size().reset_index(name='countByUserByDay')\n",
      "    19                                             # Ahora calculo la cantidad de tweets por d√≠a sin agrupar por usuario.\n",
      "    20   1597.9 MiB     -1.7 MiB           1       df_tweets['countByDay'] = df_tweets.groupby('date')['countByUserByDay'].transform('sum')\n",
      "    21                                         \n",
      "    22                                             # Con la funci√≥n Rank marco el usuario con m√°s tweets cada d√≠a, desempato usando first.\n",
      "    23   1599.7 MiB      1.8 MiB           1       df_tweets['dailyUserRank'] = df_tweets.groupby('date')['countByUserByDay'].rank(ascending=False, method='first')\n",
      "    24                                         \n",
      "    25   1599.8 MiB      0.1 MiB           1       result = df_tweets[(df_tweets['dailyUserRank'] == 1)].sort_values(by='countByDay', ascending=False).head(10)[['date','userName']]\n",
      "    26                                             # Convierte el dataframe en una lista de tuplas\n",
      "    27   1599.8 MiB      0.0 MiB          13       result_tuple = [tuple(row) for row in result.to_records(index=False)]\n",
      "    28                                         \n",
      "    29                                             # Imprime la lista de tuplas\n",
      "    30   1599.8 MiB      0.0 MiB           1       return result_tuple\n",
      "\n",
      "\n",
      "[(datetime.date(2021, 2, 12), 'RanbirS00614606'), (datetime.date(2021, 2, 13), 'MaanDee08215437'), (datetime.date(2021, 2, 17), 'RaaJVinderkaur'), (datetime.date(2021, 2, 16), 'jot__b'), (datetime.date(2021, 2, 14), 'rebelpacifist'), (datetime.date(2021, 2, 18), 'neetuanjle_nitu'), (datetime.date(2021, 2, 15), 'jot__b'), (datetime.date(2021, 2, 20), 'MangalJ23056160'), (datetime.date(2021, 2, 23), 'Surrypuria'), (datetime.date(2021, 2, 19), 'Preetm91')]\n"
     ]
    }
   ],
   "source": [
    "resultado = q1_time(extracted_dir+file_path)\n",
    "print(resultado)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se utilizan 1560 MiB de memoria utilizando esta t√©cnica."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el caso de la optimizaci√≥n de memoria, lo que hago es no cargar el conjunto de datos al mismo tiempo en memoria. En este caso lo cargo linea a linea sacando la informaci√≥n que necesito y manteniendola en un diccionario de dias que contiene un diccionario de tweets por usuario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filename: c:\\Users\\Caruso\\source\\Latam-DE-challenge\\src\\q1_memory.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "     8    148.1 MiB    148.1 MiB           1   @profile\n",
      "     9                                         def q1_memory(file_path: str) -> List[Tuple[datetime.date, str]]:\n",
      "    10                                             # Utilizo un diccionario para mantener la cuenta de tweets por usuario por dia. Uso un array para guardar los dias con mas tweets al final.\n",
      "    11    148.1 MiB      0.0 MiB           1       date_counts = {}\n",
      "    12    148.1 MiB      0.0 MiB           1       top_dates = []\n",
      "    13                                         \n",
      "    14                                             # Leo el archivo json fila a fila.\n",
      "    15    153.3 MiB     -0.1 MiB           2       with open(file_path, 'r') as jsonfile:\n",
      "    16    153.4 MiB  -4710.2 MiB      117408           for row in jsonfile:\n",
      "    17    153.4 MiB  -4735.4 MiB      117407               reader = json.loads(row)\n",
      "    18                                                     #Casteo a fecha y obtengo el nombre de usuario.\n",
      "    19    153.4 MiB  -4753.1 MiB      117407               date = pd.to_datetime(reader['date']).date()\n",
      "    20    153.4 MiB  -4768.5 MiB      117407               username = pd.json_normalize(reader['user']).username[0]\n",
      "    21                                         \n",
      "    22                                                     #Si la fecha ya estaba en el diccionario, actualizo la cuenta de tweets para ese dia. Si no, agrego la fecha y el usuario con un tweet.\n",
      "    23    153.4 MiB  -4763.5 MiB      117407               if date in date_counts:\n",
      "    24    153.4 MiB  -4763.2 MiB      117394                   if username in date_counts[date]:\n",
      "    25    153.4 MiB  -2734.0 MiB       65761                       date_counts[date][username] += 1\n",
      "    26                                                         else:\n",
      "    27    153.4 MiB  -2027.4 MiB       51633                       date_counts[date][username] = 1\n",
      "    28                                                     else:\n",
      "    29    152.7 MiB     -0.3 MiB          13                   date_counts[date] = {username: 1}\n",
      "    30                                         \n",
      "    31                                             # Encontrar las 10 fechas con m√°s tweets\n",
      "    32    153.3 MiB      0.0 MiB          37       for date, user_counts in sorted(date_counts.items(), key=lambda x: sum(x[1].values()), reverse=True)[:10]:\n",
      "    33    153.3 MiB      0.0 MiB          10           max_user = max(user_counts, key=user_counts.get)\n",
      "    34    153.3 MiB      0.0 MiB          10           top_dates.append((date, max_user))\n",
      "    35                                             # Devolver los resultados en forma de tupla.\n",
      "    36    153.3 MiB      0.0 MiB          13       result_tuple = [tuple(row) for row in top_dates]\n",
      "    37                                         \n",
      "    38                                             # Imprime la lista de tuplas\n",
      "    39    153.3 MiB      0.0 MiB           1       return result_tuple\n",
      "\n",
      "\n",
      "[(datetime.date(2021, 2, 12), 'RanbirS00614606'), (datetime.date(2021, 2, 13), 'MaanDee08215437'), (datetime.date(2021, 2, 17), 'RaaJVinderkaur'), (datetime.date(2021, 2, 16), 'jot__b'), (datetime.date(2021, 2, 14), 'rebelpacifist'), (datetime.date(2021, 2, 18), 'neetuanjle_nitu'), (datetime.date(2021, 2, 15), 'jot__b'), (datetime.date(2021, 2, 20), 'MangalJ23056160'), (datetime.date(2021, 2, 23), 'Surrypuria'), (datetime.date(2021, 2, 19), 'Preetm91')]\n"
     ]
    }
   ],
   "source": [
    "resultado = q1_memory(extracted_dir+file_path)\n",
    "print(resultado)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este enfoque utiliza muy poca memoria (153 MiB) pero es realmente lento, una buena mejora para obtener un buen balance entre velocidad y consumo de memoria ser√≠a utilizar batches de filas en lugar de ir una a una.\n",
    "Cabe aclarar tambi√©n que sin utilizar Memory Profiler el tiempo es mucho menor. Aprox 6 minutos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pregunta 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para el caso en donde se optimiza el tiempo de ejecuci√≥n, decid√≠ concatenar todos los textos de la columna content y buscar en ella los emojis usando una expresi√≥n regular. Una vez identificados los emojis, se agrupan, cuentan y ordenan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('üôè', 5049), ('üòÇ', 3072), ('üöú', 2972), ('üåæ', 2182), ('üáÆüá≥', 2086), ('ü§£', 1668), ('‚úä', 1651), ('‚ù§Ô∏è', 1382), ('üôèüèª', 1317), ('üíö', 1040)]\n"
     ]
    }
   ],
   "source": [
    "resultado = q2_time(extracted_dir+file_path)\n",
    "print(resultado)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el caso de la optimizaci√≥n de memoria, sigo un principio parecido al del problema 1, en este caso no voy al extremo de ir fila a fila sino que cargo el dataframe de a 1000 rows, El uso de memoria es muy bajo como se puede ver."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filename: c:\\Users\\Caruso\\source\\Latam-DE-challenge\\src\\q2_memory.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "     6    123.2 MiB    123.2 MiB           1   @profile\n",
      "     7                                         def q2_memory(file_path: str) -> List[Tuple[str, int]]:\n",
      "     8    123.2 MiB      0.0 MiB           1       batch_size = 1000\n",
      "     9    123.8 MiB      0.7 MiB           1       extract_emoji = re.compile(emoji_rx)                   # Match a single emoji\n",
      "    10                                         \n",
      "    11                                             # Diccionario para mantener los emojis.\n",
      "    12    123.8 MiB      0.0 MiB           1       emojis = {}\n",
      "    13                                         \n",
      "    14    123.8 MiB      0.0 MiB           1       json_reader = pd.read_json(file_path, lines=True, chunksize=batch_size)\n",
      "    15    153.0 MiB   -175.1 MiB         119       for chunk in json_reader:\n",
      "    16    153.0 MiB -227863.7 MiB      117525           for tweet in chunk['content']:\n",
      "    17    153.0 MiB -308861.9 MiB      160329               for emoji in re.findall(extract_emoji, tweet):\n",
      "    18    153.0 MiB -81219.3 MiB       42922                   if emoji in emojis:\n",
      "    19    153.0 MiB -79977.1 MiB       42052                       emojis[emoji] += 1\n",
      "    20                                                         else:\n",
      "    21    153.0 MiB  -1243.5 MiB         870                       emojis[emoji] = 1\n",
      "    22                                             # Sort the dictionary by values in descending order\n",
      "    23    148.4 MiB     -4.7 MiB        1741       sorted_dict = dict(sorted(emojis.items(), key=lambda item: item[1], reverse=True))\n",
      "    24                                         \n",
      "    25                                             # Get the top 10 items\n",
      "    26    148.3 MiB     -0.1 MiB           1       top_10_items = dict(list(sorted_dict.items())[:10])\n",
      "    27                                         \n",
      "    28    148.3 MiB      0.0 MiB           1       result_tuple = list(top_10_items.items())\n",
      "    29    148.3 MiB      0.0 MiB           1       return result_tuple\n",
      "\n",
      "\n",
      "[('üôè', 5049), ('üòÇ', 3072), ('üöú', 2972), ('üåæ', 2182), ('üáÆüá≥', 2086), ('ü§£', 1668), ('‚úä', 1651), ('‚ù§Ô∏è', 1382), ('üôèüèª', 1317), ('üíö', 1040)]\n"
     ]
    }
   ],
   "source": [
    "resultado = q2_memory(extracted_dir+file_path)\n",
    "print(resultado)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pregunta 3"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
