{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DE Challenge - Ismael Umpierrez"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocesamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para ejecutar este archivo puede ser necesario instalar las siguientes dependencias:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install gdown pandas google.cloud.storage google-cloud-bigquery memory_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gdown\n",
    "import pandas as pd\n",
    "import zipfile\n",
    "from q1_time import q1_time\n",
    "from q1_memory import q1_memory\n",
    "from q1_bigquery import q1_time as q1_bigquery\n",
    "from q2_time import q2_time\n",
    "from q2_memory import q2_memory\n",
    "\n",
    "from pandas.io import gbq\n",
    "from google.cloud import storage, bigquery"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtengo el archivo desde google drive y lo descomprimo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"farmers-protest-tweets-2021-2-4.json\"\n",
    "url = f'https://drive.google.com/uc?id=1ig2ngoXFTxP5Pa8muXo02mDTFexZzsis'\n",
    "extracted_dir = 'working/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descargo de GDrive\n",
    "gdown.download(url, 'working/tweets.zip', quiet=True)\n",
    "\n",
    "# Extraigo el contenido del archivo\n",
    "with zipfile.ZipFile('working/tweets.zip', 'r') as zip_ref:\n",
    "    zip_ref.extractall(extracted_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Subo los datos del archivo a Google Cloud Storage para poder trabajar con él desde bigquery."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables para conexión a Google Cloud Storage\n",
    "project_id = \"dechallenge\"\n",
    "bucket_name = \"dechallenge-tweets\"\n",
    "keyfile_path = \"..\\creds\\dechallenge-51f78ddf0bb6.json\"  # JSON key file\n",
    "\n",
    "storage_client = storage.Client.from_service_account_json(keyfile_path, project=project_id)\n",
    "bucket = storage_client.bucket(bucket_name)\n",
    "\n",
    "# Creo un blob con el nombre del archivo\n",
    "destination_blob_name = file_path\n",
    "blob = bucket.blob(destination_blob_name)\n",
    "\n",
    "# Subo el archivo a Google Cloud Storage\n",
    "blob.upload_from_filename(extracted_dir+file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Subo el archivo que está en un blob de GCS a una tabla de bigquery."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El dataset dechallenge.tweets ya existe.\n",
      "Loaded 117405 rows.\n"
     ]
    }
   ],
   "source": [
    "# Crea un cliente de BigQuery\n",
    "client = bigquery.Client.from_service_account_json(keyfile_path, project=project_id)\n",
    "\n",
    "# Especifica el ID del proyecto y el ID del nuevo dataset\n",
    "dataset_id = \"tweets\"\n",
    "\n",
    "# Crea el dataset\n",
    "dataset = bigquery.Dataset(f\"{project_id}.{dataset_id}\")\n",
    "dataset_ref = client.dataset(dataset_id, project=project_id)\n",
    "\n",
    "# Verifica si el dataset ya existe para no volver a crearlo.\n",
    "if not client.get_dataset(dataset_ref, retry=bigquery.DEFAULT_RETRY):\n",
    "    # Si el dataset no existe, crea el dataset\n",
    "    dataset = bigquery.Dataset(dataset_ref)\n",
    "    client.create_dataset(dataset)\n",
    "    print(f\"Dataset {project_id}.{dataset_id} creado con éxito.\")\n",
    "else:\n",
    "    print(f\"El dataset {project_id}.{dataset_id} ya existe.\")\n",
    "\n",
    "job_config = bigquery.LoadJobConfig(\n",
    "    autodetect=True,\n",
    "    write_disposition=\"WRITE_TRUNCATE\",\n",
    "    source_format=bigquery.SourceFormat.NEWLINE_DELIMITED_JSON,\n",
    "    max_bad_records=10,\n",
    ")\n",
    "uri = \"gs://dechallenge-tweets/farmers-protest-tweets-2021-2-4.json\"\n",
    "\n",
    "load_job = client.load_table_from_uri(\n",
    "    uri,\n",
    "    \"dechallenge.tweets.farmers_protest_tweets\",\n",
    "    location=\"US\",\n",
    "    job_config=job_config,\n",
    ")  # Make an API request.\n",
    "\n",
    "load_job.result()  # Waits for the job to complete.\n",
    "\n",
    "destination_table = client.get_table(\"dechallenge.tweets.farmers_protest_tweets\")\n",
    "print(\"Loaded {} rows.\".format(destination_table.num_rows))\n",
    "\n",
    "# Ejecuta la consulta de BigQuery"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pregunta 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primero quiero ver cuanto demora una solución realizada con BigQuery, haciendo uso de las capacidades que nos brinda el procesamiento distribuido en la nube."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(datetime.date(2021, 2, 12), 'RanbirS00614606'), (datetime.date(2021, 2, 13), 'MaanDee08215437'), (datetime.date(2021, 2, 17), 'RaaJVinderkaur'), (datetime.date(2021, 2, 16), 'jot__b'), (datetime.date(2021, 2, 14), 'rebelpacifist'), (datetime.date(2021, 2, 18), 'neetuanjle_nitu'), (datetime.date(2021, 2, 15), 'jot__b'), (datetime.date(2021, 2, 20), 'MangalJ23056160'), (datetime.date(2021, 2, 23), 'Surrypuria'), (datetime.date(2021, 2, 19), 'Preetm91')]\n"
     ]
    }
   ],
   "source": [
    "resultado = q1_bigquery(client)\n",
    "# Imprime la lista de tuplas\n",
    "print(resultado)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como vemos el resultado es muy rápido, pero tenemos que tener en cuenta el overhead de subir los archivos al cloud storage y a una tabla de bigquery."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De todas maneras, para el problema 1 quería utilizar pandas de manera de tener un benchmark contra lo que comparar. Además quiero también ver cual es el uso de memoria de esta solución de \"fuerza bruta\" para tener una idea del impacto de la solución que optimiza memoria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filename: c:\\Users\\Caruso\\source\\Latam-DE-challenge\\src\\q1_time.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "     6    142.0 MiB    142.0 MiB           1   @profile\n",
      "     7                                         def q1_time(file_path: str) -> List[Tuple[datetime.date, str]]:\n",
      "     8                                             # Lee el archivo json\n",
      "     9   1609.9 MiB   1467.8 MiB           1       df_tweets_source = pd.read_json(file_path, lines=True)\n",
      "    10                                         \n",
      "    11                                             # Tengo que normalizar el objeto user para poder acceder a sus atributos, en particular me interesa el username.\n",
      "    12                                             # Además, Transformo la columna 'date' a tipo datetime\n",
      "    13   1611.7 MiB      7.2 MiB           2       df_tweets = df_tweets_source.assign(\n",
      "    14   1599.9 MiB    -10.0 MiB           1               userName=pd.json_normalize(df_tweets_source['user']).username,\n",
      "    15   1604.5 MiB      4.6 MiB           1               date=pd.to_datetime(df_tweets_source['date']).dt.date)\n",
      "    16                                         \n",
      "    17                                             # Agrupo por 'userName' y 'date' y cuento las filas para saber cuantos tweets hizo cada usuario por día.\n",
      "    18   1599.6 MiB    -12.1 MiB           1       df_tweets = df_tweets.groupby(['userName','date']).size().reset_index(name='countByUserByDay')\n",
      "    19                                             # Ahora calculo la cantidad de tweets por día sin agrupar por usuario.\n",
      "    20   1597.9 MiB     -1.7 MiB           1       df_tweets['countByDay'] = df_tweets.groupby('date')['countByUserByDay'].transform('sum')\n",
      "    21                                         \n",
      "    22                                             # Con la función Rank marco el usuario con más tweets cada día, desempato usando first.\n",
      "    23   1599.7 MiB      1.8 MiB           1       df_tweets['dailyUserRank'] = df_tweets.groupby('date')['countByUserByDay'].rank(ascending=False, method='first')\n",
      "    24                                         \n",
      "    25   1599.8 MiB      0.1 MiB           1       result = df_tweets[(df_tweets['dailyUserRank'] == 1)].sort_values(by='countByDay', ascending=False).head(10)[['date','userName']]\n",
      "    26                                             # Convierte el dataframe en una lista de tuplas\n",
      "    27   1599.8 MiB      0.0 MiB          13       result_tuple = [tuple(row) for row in result.to_records(index=False)]\n",
      "    28                                         \n",
      "    29                                             # Imprime la lista de tuplas\n",
      "    30   1599.8 MiB      0.0 MiB           1       return result_tuple\n",
      "\n",
      "\n",
      "[(datetime.date(2021, 2, 12), 'RanbirS00614606'), (datetime.date(2021, 2, 13), 'MaanDee08215437'), (datetime.date(2021, 2, 17), 'RaaJVinderkaur'), (datetime.date(2021, 2, 16), 'jot__b'), (datetime.date(2021, 2, 14), 'rebelpacifist'), (datetime.date(2021, 2, 18), 'neetuanjle_nitu'), (datetime.date(2021, 2, 15), 'jot__b'), (datetime.date(2021, 2, 20), 'MangalJ23056160'), (datetime.date(2021, 2, 23), 'Surrypuria'), (datetime.date(2021, 2, 19), 'Preetm91')]\n"
     ]
    }
   ],
   "source": [
    "resultado = q1_time(extracted_dir+file_path)\n",
    "print(resultado)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se utilizan 1560 MiB de memoria utilizando esta técnica."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el caso de la optimización de memoria, lo que hago es no cargar el conjunto de datos al mismo tiempo en memoria. En este caso lo cargo linea a linea sacando la información que necesito y manteniendola en un diccionario de dias que contiene un diccionario de tweets por usuario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filename: c:\\Users\\Caruso\\source\\Latam-DE-challenge\\src\\q1_memory.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "     8    148.1 MiB    148.1 MiB           1   @profile\n",
      "     9                                         def q1_memory(file_path: str) -> List[Tuple[datetime.date, str]]:\n",
      "    10                                             # Utilizo un diccionario para mantener la cuenta de tweets por usuario por dia. Uso un array para guardar los dias con mas tweets al final.\n",
      "    11    148.1 MiB      0.0 MiB           1       date_counts = {}\n",
      "    12    148.1 MiB      0.0 MiB           1       top_dates = []\n",
      "    13                                         \n",
      "    14                                             # Leo el archivo json fila a fila.\n",
      "    15    153.3 MiB     -0.1 MiB           2       with open(file_path, 'r') as jsonfile:\n",
      "    16    153.4 MiB  -4710.2 MiB      117408           for row in jsonfile:\n",
      "    17    153.4 MiB  -4735.4 MiB      117407               reader = json.loads(row)\n",
      "    18                                                     #Casteo a fecha y obtengo el nombre de usuario.\n",
      "    19    153.4 MiB  -4753.1 MiB      117407               date = pd.to_datetime(reader['date']).date()\n",
      "    20    153.4 MiB  -4768.5 MiB      117407               username = pd.json_normalize(reader['user']).username[0]\n",
      "    21                                         \n",
      "    22                                                     #Si la fecha ya estaba en el diccionario, actualizo la cuenta de tweets para ese dia. Si no, agrego la fecha y el usuario con un tweet.\n",
      "    23    153.4 MiB  -4763.5 MiB      117407               if date in date_counts:\n",
      "    24    153.4 MiB  -4763.2 MiB      117394                   if username in date_counts[date]:\n",
      "    25    153.4 MiB  -2734.0 MiB       65761                       date_counts[date][username] += 1\n",
      "    26                                                         else:\n",
      "    27    153.4 MiB  -2027.4 MiB       51633                       date_counts[date][username] = 1\n",
      "    28                                                     else:\n",
      "    29    152.7 MiB     -0.3 MiB          13                   date_counts[date] = {username: 1}\n",
      "    30                                         \n",
      "    31                                             # Encontrar las 10 fechas con más tweets\n",
      "    32    153.3 MiB      0.0 MiB          37       for date, user_counts in sorted(date_counts.items(), key=lambda x: sum(x[1].values()), reverse=True)[:10]:\n",
      "    33    153.3 MiB      0.0 MiB          10           max_user = max(user_counts, key=user_counts.get)\n",
      "    34    153.3 MiB      0.0 MiB          10           top_dates.append((date, max_user))\n",
      "    35                                             # Devolver los resultados en forma de tupla.\n",
      "    36    153.3 MiB      0.0 MiB          13       result_tuple = [tuple(row) for row in top_dates]\n",
      "    37                                         \n",
      "    38                                             # Imprime la lista de tuplas\n",
      "    39    153.3 MiB      0.0 MiB           1       return result_tuple\n",
      "\n",
      "\n",
      "[(datetime.date(2021, 2, 12), 'RanbirS00614606'), (datetime.date(2021, 2, 13), 'MaanDee08215437'), (datetime.date(2021, 2, 17), 'RaaJVinderkaur'), (datetime.date(2021, 2, 16), 'jot__b'), (datetime.date(2021, 2, 14), 'rebelpacifist'), (datetime.date(2021, 2, 18), 'neetuanjle_nitu'), (datetime.date(2021, 2, 15), 'jot__b'), (datetime.date(2021, 2, 20), 'MangalJ23056160'), (datetime.date(2021, 2, 23), 'Surrypuria'), (datetime.date(2021, 2, 19), 'Preetm91')]\n"
     ]
    }
   ],
   "source": [
    "resultado = q1_memory(extracted_dir+file_path)\n",
    "print(resultado)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este enfoque utiliza muy poca memoria (153 MiB) pero es realmente lento, una buena mejora para obtener un buen balance entre velocidad y consumo de memoria sería utilizar batches de filas en lugar de ir una a una.\n",
    "Cabe aclarar también que sin utilizar Memory Profiler el tiempo es mucho menor. Aprox 6 minutos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pregunta 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para el caso en donde se optimiza el tiempo de ejecución, decidí concatenar todos los textos de la columna content y buscar en ella los emojis usando una expresión regular. Una vez identificados los emojis, se agrupan, cuentan y ordenan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('🙏', 5049), ('😂', 3072), ('🚜', 2972), ('🌾', 2182), ('🇮🇳', 2086), ('🤣', 1668), ('✊', 1651), ('❤️', 1382), ('🙏🏻', 1317), ('💚', 1040)]\n"
     ]
    }
   ],
   "source": [
    "resultado = q2_time(extracted_dir+file_path)\n",
    "print(resultado)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el caso de la optimización de memoria, sigo un principio parecido al del problema 1, en este caso no voy al extremo de ir fila a fila sino que cargo el dataframe de a 1000 rows, El uso de memoria es muy bajo como se puede ver."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filename: c:\\Users\\Caruso\\source\\Latam-DE-challenge\\src\\q2_memory.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "     6    123.2 MiB    123.2 MiB           1   @profile\n",
      "     7                                         def q2_memory(file_path: str) -> List[Tuple[str, int]]:\n",
      "     8    123.2 MiB      0.0 MiB           1       batch_size = 1000\n",
      "     9    123.8 MiB      0.7 MiB           1       extract_emoji = re.compile(emoji_rx)                   # Match a single emoji\n",
      "    10                                         \n",
      "    11                                             # Diccionario para mantener los emojis.\n",
      "    12    123.8 MiB      0.0 MiB           1       emojis = {}\n",
      "    13                                         \n",
      "    14    123.8 MiB      0.0 MiB           1       json_reader = pd.read_json(file_path, lines=True, chunksize=batch_size)\n",
      "    15    153.0 MiB   -175.1 MiB         119       for chunk in json_reader:\n",
      "    16    153.0 MiB -227863.7 MiB      117525           for tweet in chunk['content']:\n",
      "    17    153.0 MiB -308861.9 MiB      160329               for emoji in re.findall(extract_emoji, tweet):\n",
      "    18    153.0 MiB -81219.3 MiB       42922                   if emoji in emojis:\n",
      "    19    153.0 MiB -79977.1 MiB       42052                       emojis[emoji] += 1\n",
      "    20                                                         else:\n",
      "    21    153.0 MiB  -1243.5 MiB         870                       emojis[emoji] = 1\n",
      "    22                                             # Sort the dictionary by values in descending order\n",
      "    23    148.4 MiB     -4.7 MiB        1741       sorted_dict = dict(sorted(emojis.items(), key=lambda item: item[1], reverse=True))\n",
      "    24                                         \n",
      "    25                                             # Get the top 10 items\n",
      "    26    148.3 MiB     -0.1 MiB           1       top_10_items = dict(list(sorted_dict.items())[:10])\n",
      "    27                                         \n",
      "    28    148.3 MiB      0.0 MiB           1       result_tuple = list(top_10_items.items())\n",
      "    29    148.3 MiB      0.0 MiB           1       return result_tuple\n",
      "\n",
      "\n",
      "[('🙏', 5049), ('😂', 3072), ('🚜', 2972), ('🌾', 2182), ('🇮🇳', 2086), ('🤣', 1668), ('✊', 1651), ('❤️', 1382), ('🙏🏻', 1317), ('💚', 1040)]\n"
     ]
    }
   ],
   "source": [
    "resultado = q2_memory(extracted_dir+file_path)\n",
    "print(resultado)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pregunta 3"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
